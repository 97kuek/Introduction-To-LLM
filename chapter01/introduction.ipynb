{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# 第1章 はじめに"],"metadata":{"id":"04lbnAjUp700"}},{"cell_type":"markdown","source":["## 1.1 transformersを使って自然言語処理を解いてみよう\n","大規模言語モデルをプログラムから扱うための標準的なライブラリが**transformers**\n","<br>\n","Pythonのパッケージマネージャであるpipコマンドを使用してtransformersをインストールする。"],"metadata":{"id":"FW8T2irOZ92X"}},{"cell_type":"code","source":["!pip install transformers[ja,sentencepiece,torch]"],"metadata":{"id":"0nHAJazMp7CC","colab":{"base_uri":"https://localhost:8080/"},"outputId":"cef2ee29-ddcb-44cc-f244-ba8394446e6e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers[ja,sentencepiece,torch] in /usr/local/lib/python3.12/dist-packages (4.56.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers[ja,sentencepiece,torch]) (3.19.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers[ja,sentencepiece,torch]) (0.35.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers[ja,sentencepiece,torch]) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers[ja,sentencepiece,torch]) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers[ja,sentencepiece,torch]) (6.0.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers[ja,sentencepiece,torch]) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers[ja,sentencepiece,torch]) (2.32.4)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers[ja,sentencepiece,torch]) (0.22.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers[ja,sentencepiece,torch]) (0.6.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers[ja,sentencepiece,torch]) (4.67.1)\n","Collecting fugashi>=1.0 (from transformers[ja,sentencepiece,torch])\n","  Downloading fugashi-1.5.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (7.3 kB)\n","Collecting ipadic<2.0,>=1.0.0 (from transformers[ja,sentencepiece,torch])\n","  Downloading ipadic-1.0.0.tar.gz (13.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting unidic-lite>=1.0.7 (from transformers[ja,sentencepiece,torch])\n","  Downloading unidic-lite-1.0.8.tar.gz (47.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.4/47.4 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting unidic>=1.0.2 (from transformers[ja,sentencepiece,torch])\n","  Downloading unidic-1.1.0.tar.gz (7.7 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting sudachipy>=0.6.6 (from transformers[ja,sentencepiece,torch])\n","  Downloading SudachiPy-0.6.10-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting sudachidict-core>=20220729 (from transformers[ja,sentencepiece,torch])\n","  Downloading sudachidict_core-20250825-py3-none-any.whl.metadata (2.7 kB)\n","Collecting rhoknp<1.3.1,>=1.1.0 (from transformers[ja,sentencepiece,torch])\n","  Downloading rhoknp-1.3.0-py3-none-any.whl.metadata (7.8 kB)\n","Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.12/dist-packages (from transformers[ja,sentencepiece,torch]) (0.2.1)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from transformers[ja,sentencepiece,torch]) (5.29.5)\n","Requirement already satisfied: torch>=2.2 in /usr/local/lib/python3.12/dist-packages (from transformers[ja,sentencepiece,torch]) (2.8.0+cu126)\n","Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.12/dist-packages (from transformers[ja,sentencepiece,torch]) (1.10.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.26.0->transformers[ja,sentencepiece,torch]) (5.9.5)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers[ja,sentencepiece,torch]) (2025.3.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers[ja,sentencepiece,torch]) (4.15.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers[ja,sentencepiece,torch]) (1.1.10)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[ja,sentencepiece,torch]) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[ja,sentencepiece,torch]) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[ja,sentencepiece,torch]) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[ja,sentencepiece,torch]) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[ja,sentencepiece,torch]) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[ja,sentencepiece,torch]) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[ja,sentencepiece,torch]) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[ja,sentencepiece,torch]) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[ja,sentencepiece,torch]) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[ja,sentencepiece,torch]) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[ja,sentencepiece,torch]) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[ja,sentencepiece,torch]) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[ja,sentencepiece,torch]) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[ja,sentencepiece,torch]) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[ja,sentencepiece,torch]) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[ja,sentencepiece,torch]) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[ja,sentencepiece,torch]) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[ja,sentencepiece,torch]) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[ja,sentencepiece,torch]) (3.4.0)\n","Collecting wasabi<1.0.0,>=0.6.0 (from unidic>=1.0.2->transformers[ja,sentencepiece,torch])\n","  Downloading wasabi-0.10.1-py3-none-any.whl.metadata (28 kB)\n","Collecting plac<2.0.0,>=1.1.3 (from unidic>=1.0.2->transformers[ja,sentencepiece,torch])\n","  Downloading plac-1.4.5-py2.py3-none-any.whl.metadata (5.9 kB)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers[ja,sentencepiece,torch]) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers[ja,sentencepiece,torch]) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers[ja,sentencepiece,torch]) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers[ja,sentencepiece,torch]) (2025.8.3)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.2->transformers[ja,sentencepiece,torch]) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.2->transformers[ja,sentencepiece,torch]) (3.0.3)\n","Downloading fugashi-1.5.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (697 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m697.9/697.9 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading rhoknp-1.3.0-py3-none-any.whl (86 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sudachidict_core-20250825-py3-none-any.whl (72.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.2/72.2 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading SudachiPy-0.6.10-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading plac-1.4.5-py2.py3-none-any.whl (22 kB)\n","Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n","Building wheels for collected packages: ipadic, unidic, unidic-lite\n","  Building wheel for ipadic (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ipadic: filename=ipadic-1.0.0-py3-none-any.whl size=13556704 sha256=4d128f5499186e2d377ff0be77260a44a882b11fe26cca36f2db2f4faf0a464c\n","  Stored in directory: /root/.cache/pip/wheels/93/8b/55/dd5978a069678c372520847cf84ba2ec539cb41917c00a2206\n","  Building wheel for unidic (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for unidic: filename=unidic-1.1.0-py3-none-any.whl size=7403 sha256=c64dd0e696d5197ef30694eb5f7f35300f40e1fade6959f6168bbf9f35cd0589\n","  Stored in directory: /root/.cache/pip/wheels/cb/04/a2/659428f84ed1fa7257f8efb544b36043dd37a7c419e8ca711a\n"]}]},{"cell_type":"code","source":["from transformers import pipeline"],"metadata":{"id":"D27ZqHlIWF8j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"9ctelxmNRgSL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 1.1.1 文書分類\n","ここでは**感情分析**を行う。感情分析モデルlim-book/bert-base-japanese-v3-marc.jaは通販サイトのレビュー記事で訓練されており、テキストがpositiveであるかnegativeであるかを予測できる。"],"metadata":{"id":"uPib24mtqDDF"}},{"cell_type":"code","source":["text_classification_pipeline = pipeline(\n","    model=\"llm-book/bert-base-japanese-v3-marc_ja\"\n",")\n","positive_text = \"世界には言葉がわからなくても感動する音楽がある。\"\n","# positive_textの極性を予測\n","print(text_classification_pipeline(positive_text)[0])"],"metadata":{"id":"NZkuITLG3P12"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# negative_textの極性を予測\n","negative_text = \"世界には言葉がでないほどひどい音楽がある。\"\n","print(text_classification_pipeline(negative_text)[0])"],"metadata":{"id":"mcZJ5gev3RBT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\"score\"は予測確率を示したものなので、上記の例からいずれも96%の高い確率で妥当なラベルを予測している。"],"metadata":{"id":"JUoDeXT8PnRj"}},{"cell_type":"markdown","source":["### 1.1.2 自然言語推論\n","**自然言語推論**は、2つのテキストの論理関係を予測するタスクであり、言語モデルの意味理解能力を評価するために使用される。"],"metadata":{"id":"KfM5ymhA1K_Y"}},{"cell_type":"code","source":["nli_pipeline = pipeline(model=\"llm-book/bert-base-japanese-v3-jnli\")\n","text = \"二人の男性がジェット機を見ています\"\n","entailment_text = \"ジェット機を見ている人が二人います\"\n","\n","# textとentailment_textの論理関係を予測\n","print(nli_pipeline({\"text\": text, \"text_pair\": entailment_text}))"],"metadata":{"id":"fdnmgdhO3hKX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\"entailment\"とは含意を意味している。"],"metadata":{"id":"WHoEDSoDQv9r"}},{"cell_type":"code","source":["contradiction_text = \"二人の男性が飛んでいます\"\n","# textとcontradiction_textの論理関係を予測\n","print(nli_pipeline({\"text\": text, \"text_pair\": contradiction_text}))"],"metadata":{"id":"ap_L_bCj3n6s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\"contradiction\"は矛盾を意味している。"],"metadata":{"id":"EDBDPxPWQfWb"}},{"cell_type":"code","source":["neutral_text = \"2人の男性が、白い飛行機を眺めています\"\n","# textとneutral_textの論理関係を予測\n","print(nli_pipeline({\"text\": text, \"text_pair\": neutral_text}))"],"metadata":{"id":"RFGjb75u3pwf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\"neutral\"は中立であり、含意とも矛盾とも取れないときに出力される。"],"metadata":{"id":"3qN2t8ngQlJY"}},{"cell_type":"markdown","source":["### 1.1.3 意味的類似度計算\n","**意味的類似度計算**は2つのテキストの意味が似ている度合いをスコアとして予測するタスクである。**情報検索**や**複数テキストの内容の整合性**を確認する際に役に立つ。このモデルは与えられた2つのテキストの意味的類似度を0から5の範囲で予測する。"],"metadata":{"id":"EMLd6A6ZqFLP"}},{"cell_type":"code","source":["text_sim_pipeline = pipeline(\n","    model=\"llm-book/bert-base-japanese-v3-jsts\",\n","    function_to_apply=\"none\",\n",")\n","text = \"川べりでサーフボードを持った人たちがいます\"\n","sim_text = \"サーファーたちが川べりに立っています\"\n","# textとsim_textの類似度を計算\n","result = text_sim_pipeline({\"text\": text, \"text_pair\": sim_text})\n","print(result[\"score\"])"],"metadata":{"id":"QnrEeVmNSLRR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dissim_text = \"トイレの壁に黒いタオルがかけられています\"\n","# textとdissim_textの類似度を計算\n","result = text_sim_pipeline({\"text\": text, \"text_pair\": dissim_text})\n","print(result[\"score\"])"],"metadata":{"id":"LPceX7ro33tp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"3AebnifySVgD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["また**chapter08**ではテキストの意味をベクトルで表現する**文埋め込み**モデルを紹介する。このモデルから得られるテキストのベクトルのコサイン類似度を意味的類似度とみなすことができる。コサイン類似度の値の範囲は-1から1であることに注意して、以下のコードを実行してみる。"],"metadata":{"id":"GczVMk6NR4ro"}},{"cell_type":"code","source":["from torch.nn.functional import cosine_similarity\n","\n","sim_enc_pipeline = pipeline(\n","    model=\"llm-book/bert-base-japanese-v3-unsup-simcse-jawiki\",\n","    task=\"feature-extraction\",\n",")\n","\n","# textとsim_textのベクトルを獲得\n","text_emb = sim_enc_pipeline(text, return_tensors=True)[0][0]\n","sim_emb = sim_enc_pipeline(sim_text, return_tensors=True)[0][0]\n","# textとsim_textの類似度を計算\n","sim_pair_score = cosine_similarity(text_emb, sim_emb, dim=0)\n","print(sim_pair_score.item())"],"metadata":{"id":"axtkpgoNkOnc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# dissim_textのベクトルを獲得\n","dissim_emb = sim_enc_pipeline(dissim_text, return_tensors=True)[0][0]\n","# textとdissim_textの類似度を計算\n","dissim_pair_score = cosine_similarity(text_emb, dissim_emb, dim=0)\n","print(dissim_pair_score.item())"],"metadata":{"id":"PPyQ9w9-llsf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 1.1.4 固有表現認識\n","**固有表現認識**とはテキストに含まれる固有表現を抽出するタスクである。**テキストデータから必要な情報を抽出するための基本的なタスクの一つである**。\n","<br>\n","試しに「大谷翔平は岩手県水沢市出身のプロ野球選手」という文から固有表現を抽出してみる。"],"metadata":{"id":"Yy836I6G1N2-"}},{"cell_type":"code","source":["from pprint import pprint\n","\n","ner_pipeline = pipeline(\n","    model=\"llm-book/bert-base-japanese-v3-ner-wikipedia-dataset\",\n","    aggregation_strategy=\"simple\",\n",")\n","text = \"大谷翔平は岩手県水沢市出身のプロ野球選手\"\n","# text中の固有表現を抽出\n","pprint(ner_pipeline(text))"],"metadata":{"id":"RQisi4M5pDSk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 1.1.5 要約生成\n","**要約生成**は比較的長い文章から短い要約を生成するタスクである。"],"metadata":{"id":"NvEGn3uN1QuD"}},{"cell_type":"code","source":["text2text_pipeline = pipeline(\n","    model=\"llm-book/t5-base-long-livedoor-news-corpus\"\n",")\n","article = \"ついに始まった３連休。テレビを見ながら過ごしている人も多いのではないだろうか？　今夜オススメなのは何と言っても、NHKスペシャル「世界を変えた男 スティーブ・ジョブズ」だ。実は知らない人も多いジョブズ氏の養子に出された生い立ちや、アップル社から一時追放されるなどの経験。そして、彼が追い求めた理想の未来とはなんだったのか、ファンならずとも気になる内容になっている。 今年、亡くなったジョブズ氏の伝記は日本でもベストセラーになっている。今後もアップル製品だけでなく、世界でのジョブズ氏の影響は大きいだろうと想像される。ジョブズ氏のことをあまり知らないという人もこの機会にぜひチェックしてみよう。 世界を変えた男　スティーブ・ジョブズ（NHKスペシャル）\"\n","# articleの要約を生成\n","print(text2text_pipeline(article)[0][\"generated_text\"])"],"metadata":{"id":"GvKcGPGio9ks"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["自然言語処理にはこれ以外に多くの機能を兼ね備えている。\n","- 機械翻訳 : ある言語で記述されたテキストを別の言語に翻訳するタスク\n","- 対話システム : コンピュータが人間と対話するタスク\n","- 形態素解析 : 意味を持つ最小の単位である**形態素**に文を分割して解析する処理\n","- 共参照解析 : 異なる名詞が同一のものを指しているかどうかを識別する処理"],"metadata":{"id":"JzoOy3kLTS2-"}},{"cell_type":"markdown","source":["## 1.2 transformersの基本的な使い方\n","基本的に**Auto Classes**というクラス群を用いる。適切な実装を自動的に選択してくれる。\n","<br>\n","大規模言語モデルを含むおおくの自然言語処理のモデルでは、テキストを細かい単位に分割してからモデルに入力する。モデルが扱う基本的な単位を**トークン**、トークンに分割する処理を**トークナイゼーション**、トークン単位に分割する実装を**トークナイザ**と呼ぶ。以下のコードはAutoTokenizerを使ってテキストをトークンに分割する処理である。"],"metadata":{"id":"2rsE2WYDp2g-"}},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","\n","# AutoTokenizerでトークナイザをロードする\n","tokenizer = AutoTokenizer.from_pretrained(\"abeja/gpt2-large-japanese\")\n","# 入力文をトークンに分割する\n","tokenizer.tokenize(\"今日は天気が良いので\")"],"metadata":{"id":"8KpsxTQ1o-D0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM\n","\n","# 生成を行うモデルであるAutoModelForCausalLMを使ってモデルをロードする\n","model = AutoModelForCausalLM.from_pretrained(\n","    \"abeja/gpt2-large-japanese\"\n",")\n","# トークナイザを使ってモデルへの入力を作成する\n","inputs = tokenizer(\"今日は天気が良いので\", return_tensors=\"pt\")\n","# 後続のテキストを予測\n","outputs = model.generate(\n","    **inputs,\n","    max_length=15,  # 生成する最大トークン数を15に指定\n","    pad_token_id=tokenizer.pad_token_id  # パディングのトークンIDを指定\n",")\n","# generate関数の出力をテキストに変換する\n","generated_text = tokenizer.decode(\n","    outputs[0], skip_special_tokens=True\n",")\n","print(generated_text)"],"metadata":{"id":"EWamognKp0Jt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 1.3 単語埋め込みとニューラルネットワークの基礎\n","単語の意味をコンピュータで扱う方法である**単語埋め込み**についてみていく。\n","<br>\n","まず、単語の意味をコンピュータに教えるにはどうしたらよいか。従来の自然言語処理では**WordNet**に代表されるような人手で佐生精された単語同士の関係を記述した辞書を使って、コンピュータで意味を表現する方法が研究されてきた。\n","<br>\n","こうしたなかで、単語の意味を表現したベクトルを大規模なテキストから学習できることを示したニューラルネットワークが2013年に発表された。**単語ベクトル**とよび、自然言語処理に用いるために構築されたテキストを**コーパス**と呼ぶ。これは、ある単語の意味は周辺に出現する単語によって表せると考える**分布仮設**に基づいて設計されている。\n","<br>\n","例としてskip-gramを用いて「今日こたつでみかんを食べる」という文から単語の意味を学習することを考える。skip-gramでは、中央の単語から左側と右側の窓幅文の周辺単語を予測することで学習が行われる。単語の予測確率は**softmax function**を用いて計算される。出力は要素の合計が1であるため確率分布ととらえられる。\n","<br>\n","$$\n","softmax_m(c)=\\frac{\\exp(c_m)}{\\Sigma_{k=1}^{K}\\exp(c_k)}\n","$$\n","<br>\n","損失関数の最小化は**勾配法**を用いて行うことができる(勾配降下法)。\n","<br>\n","$$\n","\\theta^{(t+1)}=\\theta^{(t)}-\\alpha \\nabla_{\\theta}L(\\theta)\n","$$\n","<br>\n","ここで$\\alpha$は学習率とされ、一度にどの程度の大きさでパラメータを更新するかを制御している。ステップごとに訓練コーパス中の全単語について求めるのは計算負荷が大きいので、**ミニパッチ**をしようして、**確率的勾配降下法**により求める。\n","<br>\n","また、勾配を求める際には**誤差逆伝搬法**が標準的に用いられる。この方法は損失を**forward computation**で計算した後に、微分の連鎖律を利用して損失を逆方向に伝搬させることで各パラメータの勾配を計算する方法である。\n","<br>\n","word2vecのように、モデルをあらかじめ別のタスクで訓練することを**事前学習**、事前学習したモデルを適用する先のタスクのことを**下流タスク**と呼ぶ。ある解きたいタスクに対して別の方法で学習したモデルを転用する方式を**転移学習**という。入力から自動的に予測するラベルを生成して学習を行う方式を**自己教師あり学習**という。"],"metadata":{"id":"J__ji5-WctDa"}},{"cell_type":"markdown","source":["## 1.4 大規模言語モデルとは\n","word2vecの登場以降、文脈を考慮した単語埋め込みである**文脈化単語埋め込み**を大規模なコーパスから自己教師あり学習で獲得するモデルが提案された。これとほぼ同時期に、機械翻訳のモデルとして後述する**Transformer**という優れたニューラルネットワークが提案された。"],"metadata":{"id":"N9wUb_oJiI_l"}}]}